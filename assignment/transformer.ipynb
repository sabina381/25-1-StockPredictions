{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "path = './datasets/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
    "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "\n",
    "input_shape = train_data[0][0].shape\n",
    "output_shape = len(train_data.classes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, device, max_len=512, d_model=16):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        # how should we fill out self.pos_enc?\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_enc = torch.zeros(max_len,d_model,requires_grad=False) # [max_len, d_model]\n",
    "        pos = torch.arange(1, max_len+1, 1, device=device, requires_grad=False).reshape(-1, 1) # [max_len, 1]\n",
    "        i = torch.arange(1, d_model // 2 + 1, 1, device=device, requires_grad=False) # [d_model/2]\n",
    "        pos_value = 10000 ** (-2 * i / d_model) # [d_model/2]\n",
    "        self.pos_enc[:, 0::2] = torch.cos(pos * pos_value)\n",
    "        self.pos_enc[:, 1::2] = torch.sin(pos * pos_value)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # fill out here\n",
    "        \"\"\"\n",
    "        x: transformed input embedding where x.shape = [batch_size, seq_len, data_dim]\n",
    "        \"\"\"\n",
    "        return x + self.pos_enc[:x.shape[1], :].unsqueeze(0) # 배치 차원 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "# refer to Section 3.2.1 and Fig 2 (left) in the paper\n",
    "\n",
    "    def __init__(self, d_ff):\n",
    "        super().__init__()\n",
    "        # there is nothing to do here\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "    def forward(self, q, k, v, mask=False):\n",
    "        # fill out here\n",
    "        # compute attention value based on transformed query, key, value where mask is given conditionally\n",
    "        \"\"\"\n",
    "        q, k, v = transformed query, key, value\n",
    "        q.shape, k.shape, v.shpae = [batch_size, num_head, seq_len, d_ff=d_model/num_head]\n",
    "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
    "        \"\"\"\n",
    "        # attention 계산\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))/(self.d_ff ** 0.5) # [batch_size, num_head, seq_len, seq_len]\n",
    "\n",
    "        # 마스킹 추가\n",
    "        if mask:\n",
    "            mask_array = torch.tensor(np.triu(np.full(scores.shape, fill_value= -np.inf), k= 1), dtype=torch.float32)\n",
    "            scores = scores + mask_array\n",
    "        \n",
    "        weights = F.softmax(scores) # [batch_size, num_head, seq_len, seq_len]\n",
    "        attention_value = torch.matmul(weights, v) # [batch_size, num_head, seq_len, d_ff]\n",
    "\n",
    "        return attention_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=16, num_head=4):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_head == 0, \"check if d_model is divisible by num_head\"\n",
    "\n",
    "        # dimensions\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_ff = d_model//num_head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(self.d_ff)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, dec, mask=False, cross=None):\n",
    "        # fill out here\n",
    "        # compute multi-head attention value\n",
    "        # here, query, key, value are pre-transformed, so you need to transfrom them in this module\n",
    "        \"\"\"\n",
    "        q, k, v = pre-transformed query, key, value\n",
    "        q.shape, k.shape, v.shape = [batch_size, seq_len, d_model]\n",
    "        mask = masking matrix, if the index has value False, kill the value; else, leave the value\n",
    "        \"\"\"\n",
    "        q = self.w_q(cross) if cross!=None else self.w_q(dec)\n",
    "        k = self.w_k(dec)\n",
    "        v = self.w_v(dec)\n",
    "\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # 헤드 분할 (dim: [batch_size, num_head, seq_len, d_ff])\n",
    "        q = q.view(batch_size, -1, self.num_head, self.d_ff).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_head, self.d_ff).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_head, self.d_ff).transpose(1, 2)\n",
    "\n",
    "        # attention 계산\n",
    "        attention_value = self.attention(q, k, v, mask) # [batch_size, num_head, seq_len, d_ff]\n",
    "        \n",
    "        result = attention_value.transpose(1, 2) # [batch_size, seq_len, num_head, d_ff]\n",
    "        output = self.w_o(result.contiguous().view(batch_size, -1, self.d_model)) # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork(nn.Module):\n",
    "# refer to Section 3.3 in the paper\n",
    "# do not use torch.nn.Conv1d\n",
    "\n",
    "    def __init__(self, d_model=16, d_ff=32):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # fill out here\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = F.relu(self.linear_1(x)) # [batch_size, seq_len, d_ff]\n",
    "        output = self.linear_2(x) # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "# do not use torch.nn.LayerNorm\n",
    "\n",
    "    def __init__(self, d_model=16, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # fill out here\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        mean = torch.mean(x, dim=-1, keepdim= True)\n",
    "        var = torch.var(x, dim=-1, unbiased= False, keepdim= True)\n",
    "        \n",
    "        normed = (x - mean)/torch.sqrt(var + self.eps)\n",
    "        normed = self.linear(normed)\n",
    "\n",
    "        return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder, Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "# refer to Section 3.1 and Figure 1 in the paper\n",
    "# this is a single encoder block consists of the following\n",
    "# multi-head attention, positionwise feed forward network, residual connections, layer normalizations \n",
    "\n",
    "    def __init__(self, d_model=16, num_head=4, d_ff=32, drop_prob=.1):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        self.attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.ffn = PositionwiseFeedForwardNetwork()\n",
    "        \n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "\n",
    "        # dropout\n",
    "\n",
    "\n",
    "    def forward(self, enc):\n",
    "        # fill out here\n",
    "        '''\n",
    "        (1 layer)\n",
    "        enc -> w_q, w_k, w_v -> q, k, v\n",
    "        -> multi-head attention\n",
    "        -> residual learning, layer normalizing\n",
    "        -> FFN -> residual learning, layer normalizing => output\n",
    "        '''\n",
    "        _res = enc\n",
    "        \n",
    "        # self attention\n",
    "        attention_result = self.attention(enc) # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # residual learning + layer normalizing\n",
    "        # enc: [batch_size, seq_len, d_model]\n",
    "        sub_layer_1 = self.layer_norm1(_res + attention_result)\n",
    "\n",
    "        # FFN\n",
    "        _res = sub_layer_1\n",
    "        ffn_result = self.ffn(sub_layer_1)\n",
    "\n",
    "        # residual learning + layer normalizing\n",
    "        sub_layer_2 = self.layer_norm2(_res + ffn_result)\n",
    "        \n",
    "        return sub_layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "# refer to Section 3.1 and Figure 1 in the paper\n",
    "# this is a single decoder block consists of the following\n",
    "# mawsked multi-head attention, multi-head attention, positionwise feed forward network, residual connections, layer normalizations\n",
    "\n",
    "    def __init__(self,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_head)\n",
    "        self.ffn = PositionwiseFeedForwardNetwork()\n",
    "\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.layer_norm3 = LayerNormalization()\n",
    "\n",
    "\n",
    "    def forward(self,enc_output,dec):\n",
    "        # fill out here\n",
    "        '''\n",
    "        (1 layer)\n",
    "        dec -> w_q, w_k, w_v -> q1, k1, v1\n",
    "        -> multi-head attention (self attention)\n",
    "        -> residual learning, layer normalizing => q2\n",
    "\n",
    "        enc_output -> w_k, w_v -> k2, v2\n",
    "        q2, k2, v2 -> cross attention\n",
    "        -> residual learning, layer normalizing\n",
    "        -> FFN -> residual learning, layer normalizing => output\n",
    "        '''\n",
    "        ########## self attention\n",
    "        _res = dec\n",
    "\n",
    "        # self attention\n",
    "        self_result = self.self_attention(dec, mask=True) # [batch_size, dec_len, d_model]\n",
    "\n",
    "        # residual learning + layer normalizing\n",
    "        sub_layer_1 = self.layer_norm1(_res + self_result)\n",
    "\n",
    "        ########## cross attention\n",
    "\n",
    "        # cross attention\n",
    "        cross_result = self.cross_attention(enc_output, cross=sub_layer_1) # [batch_size, dec_len, d_model]\n",
    "\n",
    "        # residual learning + layer normalizing\n",
    "        sub_layer_2 = self.layer_norm2(sub_layer_1 + cross_result)\n",
    "         \n",
    "        # FFN\n",
    "        ffn_result = self.ffn(sub_layer_2)\n",
    "\n",
    "        # residual learning + layer normalizing\n",
    "        sub_layer_3 = self.layer_norm3(sub_layer_2 + ffn_result)\n",
    "\n",
    "        return sub_layer_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "# refer to Section 3.1 and Figure 1 in the paper\n",
    "# this is a whole encoder, i.e., the left side of Figure 1, consists of the following as well\n",
    "# input embedding, positional encoding\n",
    "    \"\"\"\n",
    "    in this homework, encoder inputs are not tokens, it is already embeddings in the input dimension\n",
    "    hence, you don't have to set input embedding layer\n",
    "    instead, you have to transform the input into the hidden dimension with single linear transformation\n",
    "    \"\"\"\n",
    "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "        \n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(device)\n",
    "\n",
    "        # 다 다른 파라미터를 가지도록 해야함!!\n",
    "        self.encoder_layer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.encoder_layer.append(EncoderLayer())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # fill out here\n",
    "        '''\n",
    "        positional encoding\n",
    "        encoder layer * n\n",
    "        '''\n",
    "        # positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # encoder layer\n",
    "        for i in range(self.num_layer):\n",
    "            hidden = self.encoder_layer[i](x)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "# refer to Section 3.1 and Figure 1 in the paper\n",
    "# this is a whole decoder, i.e., the left side of Figure 1, consists of the following as well\n",
    "# input embedding, positional encoding, linear classifier\n",
    "    \"\"\"\n",
    "    in this homework, decoder inputs are not tokens, it is already embeddings in the input dimension\n",
    "    hence, you don't have to set input embedding layer\n",
    "    instead, you have to transform the input into the hidden dimension with single linear transformation\n",
    "    \"\"\"\n",
    "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(device)\n",
    "\n",
    "        self.decoder_layer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.decoder_layer.append(DecoderLayer())\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "    def forward(self,enc_output, y):\n",
    "        # fill out here\n",
    "        '''\n",
    "        positional encoding\n",
    "        decoder layer * n\n",
    "        linear -> softmax\n",
    "        '''\n",
    "        # positional encoding\n",
    "        y = self.positional_encoding(y)\n",
    "\n",
    "        # decoder layer\n",
    "        for i in range(self.num_layer):\n",
    "            output = self.decoder_layer[i](enc_output, y)\n",
    "\n",
    "        output = F.softmax(self.w_o(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "# refer to Section 3.1 and Figure 1 in the paper\n",
    "# sum up encoder and decoder\n",
    "\n",
    "    def __init__(self,device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=32,drop_prob=.1):\n",
    "        super().__init__()\n",
    "        # fill out here\n",
    "\n",
    "        self.encoder = Encoder(device)\n",
    "        self.decoder = Decoder(device)\n",
    "\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        # fill out here\n",
    "        '''\n",
    "        x -> encoder => hidden\n",
    "        y, hidden -> decoder => dec_output\n",
    "        '''\n",
    "        hidden = self.encoder(x)\n",
    "        dec_output = self.decoder(hidden, y)\n",
    "\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오류 테스트\n",
    "gpt가 짜줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer_modules():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 테스트용 더미 데이터 생성\n",
    "    batch_size = 4\n",
    "    seq_len = 512\n",
    "    d_model = 16\n",
    "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "    y = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "\n",
    "    try:\n",
    "        # Positional Encoding 테스트\n",
    "        pe = PositionalEncoding(device, max_len=512, d_model=16)\n",
    "        out_pe = pe(x)\n",
    "        print(\"✅ PositionalEncoding 통과\")\n",
    "        \n",
    "        # 인코더 레이어 테스트\n",
    "        encoder_layer = EncoderLayer(d_model=16, num_head=4, d_ff=32)\n",
    "        out_enc_layer = encoder_layer(x)\n",
    "        print(\"✅ EncoderLayer 통과\")\n",
    "        \n",
    "        # 디코더 레이어 테스트\n",
    "        decoder_layer = DecoderLayer(d_model=16, num_head=4, d_ff=32)\n",
    "        out_dec_layer = decoder_layer(x, y)\n",
    "        print(\"✅ DecoderLayer 통과\")\n",
    "        \n",
    "        # 전체 트랜스포머 테스트\n",
    "        transformer = Transformer(device, input_dim=3, num_layer=3, \n",
    "                                max_len=512, d_model=16, num_head=4)\n",
    "        out_transformer = transformer(x, y)\n",
    "        print(\"✅ Transformer 통과\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PositionalEncoding 통과\n",
      "✅ EncoderLayer 통과\n",
      "✅ DecoderLayer 통과\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/1m8kjyh1283_znnjwmd3h7740000gn/T/ipykernel_53737/2760884911.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = F.softmax(scores) # [batch_size, num_head, seq_len, seq_len]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformer 통과\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/1m8kjyh1283_znnjwmd3h7740000gn/T/ipykernel_53737/501845941.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(self.w_o(output))\n"
     ]
    }
   ],
   "source": [
    "test_transformer_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "#### there is nothing to do from here ####\n",
    "##########################################\n",
    "\n",
    "class ScheduledOptimizer:\n",
    "\n",
    "    def __init__(self,optimizer,d_model=16,warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def update_parameter_and_learning_rate(self):\n",
    "        self.optimizer.step()\n",
    "        self.step_num += 1\n",
    "        self.lr = self.d_model**(-.5) * min(self.step_num**(-.5),self.step_num*self.warmup_steps**(-1.5))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# mps device로 변경\n",
    "\n",
    "model = Transformer(device=device,input_dim=3,num_layer=3,max_len=512,d_model=16,num_head=4,d_ff=64,drop_prob=.1).to(device)\n",
    "loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(),betas=(.9,.98),eps=1e-9)\n",
    "scheduled_optimizer = ScheduledOptimizer(optimizer,d_model=16)\n",
    "\n",
    "\n",
    "num_epoch = 100\n",
    "train_loss_list, test_loss_list = list(), list()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"num_param:\", total_params)\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    \n",
    "    ## train\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "\n",
    "        image = image.reshape(-1,3,1024).transpose(1,2)\n",
    "        x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
    "\n",
    "        y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
    "        y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
    "        \n",
    "        logit = model.forward(x,y_)\n",
    "        cost = loss(logit, y)/(3*512)\n",
    "        \n",
    "        total_loss += cost.item()\n",
    "\n",
    "        scheduled_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        scheduled_optimizer.update_parameter_and_learning_rate()\n",
    "        \n",
    "    ave_loss = total_loss/len(train_data)\n",
    "    train_loss_list.append(ave_loss)\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(\"\\nEpoch %d Train: %.3f w/ Learning Rate: %.5f\"%(i,ave_loss,scheduled_optimizer.lr))\n",
    "\n",
    "    ## test\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):\n",
    "\n",
    "            image = image.reshape(-1,3,1024).transpose(1,2)\n",
    "            x, y = image[:,:512,:].to(device), image[:,512:,:].to(device)\n",
    "\n",
    "            y_ = torch.zeros([batch_size,1,3],requires_grad=False).to(device)\n",
    "            y_ = torch.cat([y_,y[:,:-1,:]],dim=1)\n",
    "            \n",
    "            logit = model.forward(x,y_)\n",
    "            cost = loss(logit, y)/(3*512)\n",
    "\n",
    "            total_loss += cost.item()\n",
    "\n",
    "    ave_loss = total_loss/len(test_data)\n",
    "    test_loss_list.append(ave_loss)\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(\"Epoch %d Test: %.3f\"%(i,ave_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
